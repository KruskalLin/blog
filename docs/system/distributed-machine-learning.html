
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>distributed machine learning Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    
    <link rel="prev" href="./" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    

    
        
        <li class="divider"></li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../statistics/">
            
                <a href="../statistics/">
            
                    
                    STATISTICS
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.1.1" data-path="../statistics/statistics-using-R.html">
            
                <a href="../statistics/statistics-using-R.html">
            
                    
                    statistics using R
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.2" data-path="../statistics/statistics-formulas.html">
            
                <a href="../statistics/statistics-formulas.html">
            
                    
                    statistics formulas
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    
        
        <li class="divider"></li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../computer-vision/">
            
                <a href="../computer-vision/">
            
                    
                    COMPUTER VISION
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="3.1.1" data-path="../computer-vision/lane-detection.html">
            
                <a href="../computer-vision/lane-detection.html">
            
                    
                    lane detection
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.1.2" data-path="../computer-vision/stereo-matching.html">
            
                <a href="../computer-vision/stereo-matching.html">
            
                    
                    stereo matching
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.1.3" data-path="../computer-vision/lidar-fusion.html">
            
                <a href="../computer-vision/lidar-fusion.html">
            
                    
                    lidar fusion
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.1.4" data-path="../computer-vision/segmentation.html">
            
                <a href="../computer-vision/segmentation.html">
            
                    
                    segmentation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.1.5" data-path="../computer-vision/DCNN.html">
            
                <a href="../computer-vision/DCNN.html">
            
                    
                    DCNN
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    
        
        <li class="divider"></li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="../data-mining/">
            
                <a href="../data-mining/">
            
                    
                    DATA MINING
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="4.1.1" data-path="../data-mining/text-mining-procedure.html">
            
                <a href="../data-mining/text-mining-procedure.html">
            
                    
                    text mining procedure
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    
        
        <li class="divider"></li>
        
        
    
        <li class="chapter " data-level="5.1" data-path="../neural-network/">
            
                <a href="../neural-network/">
            
                    
                    NEURAL NETWORK
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="5.1.1" data-path="../neural-network/neural-network.html">
            
                <a href="../neural-network/neural-network.html">
            
                    
                    neural network
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.1.2" data-path="../neural-network/nonconvex-optimization.html">
            
                <a href="../neural-network/nonconvex-optimization.html">
            
                    
                    nonconvex optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.1.3" data-path="../neural-network/neural-attack.html">
            
                <a href="../neural-network/neural-attack.html">
            
                    
                    neural attack
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    
        
        <li class="divider"></li>
        
        
    
        <li class="chapter " data-level="6.1" data-path="../miscellaneous/">
            
                <a href="../miscellaneous/">
            
                    
                    MISCELLANEOUS
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="6.1.1" data-path="../miscellaneous/images-loading.html">
            
                <a href="../miscellaneous/images-loading.html">
            
                    
                    images loading
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.1.2" data-path="../miscellaneous/convert-mathjax.html">
            
                <a href="../miscellaneous/convert-mathjax.html">
            
                    
                    convert mathjax
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.1.3" data-path="../miscellaneous/hashmap.html">
            
                <a href="../miscellaneous/hashmap.html">
            
                    
                    hashmap
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="6.1.4" data-path="../miscellaneous/linkedhashmap.html">
            
                <a href="../miscellaneous/linkedhashmap.html">
            
                    
                    linkedhashmap
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    
        
        <li class="divider"></li>
        
        
    
        <li class="chapter " data-level="7.1" data-path="./">
            
                <a href="./">
            
                    
                    SYSTEM
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="7.1.1" data-path="distributed-machine-learning.html">
            
                <a href="distributed-machine-learning.html">
            
                    
                    distributed machine learning
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >distributed machine learning</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="distributed-machine-learning-framework">Distributed Machine Learning Framework</h2>
<p><img src="img/framework.png" alt="framework"></p>
<h2 id="partitionparallelism">Partition&amp;Parallelism</h2>
<h3 id="data-parallelism">Data Parallelism</h3>
<p>Data parallelism focuses on distributing the data across different nodes, which operate on the data in parallel. There are two ways to split data: split samples(<strong>random sampling/shuffling and dividing as mini-batch</strong>) and split dimensions(only used in dimensions that are mutually independent).</p>
<h3 id="model-parallelism">Model Parallelism</h3>
<h4 id="linear-model">Linear Model</h4>
<p>Linear models like Linear Regression or Logistic Regression have the same size of parameters as the size of dimensions and the dimensions are mutually independent (like what we mention in data parallelism). We can divide the dimensions and use <strong>coordinate descent</strong> method to optimize the model.</p>
<h4 id="neural-network">Neural Network</h4>
<p>Divide the model into different parts and allocate the task to different nodes. Every divided node would calculate the output after all of the node&#x2019;s dependences are calculated then update the parameters and send output to the next node.</p>
<p><strong>Horizontal Partition</strong>(also can be called <strong>workload parallelism</strong>)</p>
<p><img src="img/horizontal.png" alt="horizontal"></p>
<p>It partitions workload in different layers into different part (not only allocated in GPU). </p>
<p><strong>Vertical Partition</strong></p>
<p>It partitions workload in vertical into different part (e.g. <strong>group convolution</strong>).</p>
<p><strong>Mixed Partition</strong></p>
<p><img src="img/mix.png" alt="mix"></p>
<p>Put horizontal partition and vertical partition together. (e.g. <strong>DistBelief</strong>[1])</p>
<p><strong>Random Partition</strong></p>
<p>Explore the optimal partition for neural network, mainly used in model compression.</p>
<h2 id="distributed-computing-model">Distributed Computing Model</h2>
<h3 id="datamodel-parallelism">Data/Model Parallelism</h3>
<h4 id="mapreduceallreduce2">MapReduce/AllReduce[2]</h4>
<p>A MapReduce framework (or system) is usually composed of three operations (or steps):</p>
<ol>
<li><strong>Map:</strong> each worker node applies the <code>map</code> function to the local data, and writes the output to a temporary storage. A master node ensures that only one copy of the redundant input data is processed.</li>
<li><strong>Shuffle:</strong> worker nodes redistribute data based on the output keys (produced by the <code>map</code> function), such that all data belonging to one key is located on the same worker node.</li>
<li><strong>Reduce:</strong> worker nodes now process each group of output data, per key, in parallel.</li>
</ol>
<p><img src="img/mapreduce.png" alt="mapreduce"></p>
<p>When synchronizing clusters in MapReduce framework, we can use MPI(Message Passing Interface) like AllReduce.</p>
<p><img src="img/allreduce.png" alt="allreduce"></p>
<p>e.g. <strong>Hadoop</strong>, <strong>Spark</strong></p>
<h4 id="parameter-server3">Parameter Server[3]</h4>
<p>If one node in MapReduce doesn&#x2019;t work, the whole system would stop. The parameter server makes all the parameter in one server. The workers only communicate with server and there is no need for workers to communicate with each other.</p>
<p><img src="img/ps.png" alt="ps"></p>
<p>e.g. <strong>Multiverso</strong></p>
<h4 id="streaming">Streaming</h4>
<p>Conceptually, a stream is a (potentially never-ending) flow of data records, and a transformation is an operation that takes one or more streams as input, and produces one or more output streams as a result.</p>
<p>e.g. <strong>Flink</strong></p>
<h4 id="dataflowmany-can-be-accounted-as-this">Dataflow(Many can be accounted as this)</h4>
<p>dataflow programming is a programming paradigm that models a program as a directed graph of the data flowing between operations, thus implementing dataflow principles and architecture. Here we specify this as <strong>dataflow and differentiable programming</strong>, which means that every node in dataflow is responsible only for forward computing but also for backward computing.</p>
<p><img src="img/dataflow.png" alt="dataflow"></p>
<p>e.g. <strong>Pytorch</strong></p>
<h3 id="concurrent-design-pattern"><a href="https://java-design-patterns.com/patterns/" target="_blank">Concurrent Design Pattern</a></h3>
<p>Many distributed design patterns are borrowed from this.</p>
<h4 id="masterworkercan-be-seen-as-map-reduce"><a href="https://java-design-patterns.com/patterns/master-worker-pattern/" target="_blank">Master/Worker</a>(can be seen as Map-Reduce)</h4>
<p><img src="img/master.png" alt="master"></p>
<p>This pattern can be used when data can be divided into multiple parts, all of which need to go through the same computation to give a result, which need to be aggregated to get the final result.</p>
<h4 id="async-method-invocation"><a href="https://java-design-patterns.com/patterns/async-method-invocation/" target="_blank">Async Method Invocation</a></h4>
<p>Also see <a href="#Asynchronous%20Communication">Asynchronous Communication</a></p>
<p><img src="img/async.jpg" alt="async"></p>
<p>e.g. <strong>Future</strong>/<strong>Promise</strong></p>
<h4 id="half-synchalf-async"><a href="https://java-design-patterns.com/patterns/half-sync-half-async/" target="_blank">Half-Sync/Half-Async</a></h4>
<p>The Half-Sync/Half-Async pattern decouples synchronous I/O from asynchronous I/O in a system to simplify concurrent programming effort without degrading execution efficiency.</p>
<p><img src="img/half-sync-half-async.png" alt="half-sync-half-async"></p>
<h3 id="concurrent-communication-model">Concurrent Communication Model</h3>
<h4 id="actor">Actor</h4>
<p>Actors are concurrency primitives which can send messages to each other, create new Actors and determine how to respond to the next received message. They keep their own private state without sharing it, so they can affect each other only through messages. Since there is no shared state, there is no need for locks.</p>
<p><img src="img/actor.png" alt="actor"></p>
<p>e.g. <strong>Akka</strong></p>
<h4 id="csp">CSP</h4>
<p>Communicating Sequential Processes (CSP) is a paradigm which is very similar to the Actor model. It&#x2019;s also based on message-passing without sharing memory. However, CSP and Actors have these 2 key differences:</p>
<ul>
<li>Processes in CSP are anonymous, while actors have identities. So, CSP uses explicit channels for message passing, whereas with Actors you send messages directly.</li>
<li>With CSP the sender cannot transmit a message until the receiver is ready to accept it. Actors can send messages asynchronously.</li>
</ul>
<p><img src="img/csp.png" alt="csp"></p>
<h4 id="stm">STM</h4>
<p>While Actors and CSP are concurrency models which are based on message passing, Software Transactional Memory (STM) is a model which uses shared memory. It&#x2019;s an alternative to lock-based synchronization. Similarly to DB transactions, these are the main concepts:</p>
<ol>
<li>Values within a transaction can be changed, but these changes are not visible to others until the transaction is committed.</li>
<li>Errors that happened in transactions abort them and rollback all the changes.</li>
<li>If a transaction can&#x2019;t be committed due to conflicting changes, it retries until it succeeds.</li>
</ol>
<h2 id="communication">Communication</h2>
<h3 id="method">Method</h3>
<p>RPC/HTTP/MQ/Socket/&#x2026;</p>
<h3 id="pace">Pace</h3>
<h4 id="synchronous-communication">Synchronous Communication</h4>
<h4 id="bsp">BSP</h4>
<p>Bulk Synchronous Parallel computer consists of</p>
<ul>
<li>components capable of processing and/or memory transactions,</li>
<li>a network that routes messages between pairs of such components, and</li>
<li>a hardware facility that allows for the synchronisation of all or a subset of components.</li>
</ul>
<p>In distributed model, the BSP model can not only be run on multiple processors(CPU, GPU, etc.), but also can be allocated in multiple servers.</p>
<p><img src="img/bsp.png" alt="bsp"></p>
<p>e.g. <strong>Pregel</strong></p>
<h4 id="asynchronous-communication">Asynchronous Communication</h4>
<p>See <a href="#Distributed%20Machine%20Learning%20Algorithms">Distributed Machine Learning Algorithms</a></p>
<h4 id="sspstale-synchronous-parallel">SSP(Stale Synchronous Parallel)</h4>
<p><img src="img/ssp.jpg" alt="ssp"></p>
<p>Tackle the difference of pace of worker in asynchronous communication.</p>
<h2 id="aggregation">Aggregation</h2>
<p>We consider two ways of aggregation of model parameters:</p>
<ul>
<li>All parameters/output adding. This is mainly for data parallelism. e.g. MA, ADMM, SSGD, etc.</li>
<li>Partial parameters/output adding. e.g. ASGD, AADMM, D-PSGD, etc.</li>
</ul>
<p>See <a href="#Distributed%20Machine%20Learning%20Algorithms">Distributed Machine Learning Algorithms</a></p>
<h2 id="distributed-machine-learning-algorithms">Distributed Machine Learning Algorithms</h2>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Parallelism</th>
<th>Communication</th>
<th>Aggregation</th>
</tr>
</thead>
<tbody>
<tr>
<td>SSGD</td>
<td>data</td>
<td>sync</td>
<td>all</td>
</tr>
<tr>
<td>MA</td>
<td>data</td>
<td>sync</td>
<td>all</td>
</tr>
<tr>
<td>BMUF</td>
<td>data</td>
<td>sync</td>
<td>all</td>
</tr>
<tr>
<td>ADMM</td>
<td>data</td>
<td>sync</td>
<td>all</td>
</tr>
<tr>
<td>EASGD</td>
<td>data</td>
<td>sync</td>
<td>all</td>
</tr>
<tr>
<td>ASGD</td>
<td>data</td>
<td>async</td>
<td>partial</td>
</tr>
<tr>
<td>AADMM</td>
<td>data</td>
<td>async</td>
<td>partial</td>
</tr>
<tr>
<td>Hogwild!</td>
<td>data</td>
<td>async</td>
<td>partial</td>
</tr>
<tr>
<td>Cylades</td>
<td>data</td>
<td>async</td>
<td>partial</td>
</tr>
<tr>
<td>D-PSGD</td>
<td>data</td>
<td>async</td>
<td>partial</td>
</tr>
<tr>
<td>AdaDelay</td>
<td>data</td>
<td>async</td>
<td>partial</td>
</tr>
<tr>
<td>Group Convolution</td>
<td>model</td>
<td>sync</td>
<td>all</td>
</tr>
<tr>
<td>DistBelief</td>
<td>model</td>
<td>async</td>
<td>partial</td>
</tr>
</tbody>
</table>
<h3 id="synchronous-algorithm">Synchronous Algorithm</h3>
<h4 id="ssgd">SSGD</h4>
<pre><code class="lang-pseudocode">Initialize: parameter w_0, number of workers K, epochs T, learning rates \eta
for t = 0, 1, ..., T - 1:
    Pull w_t
    Random sampling mini-batch data i
    Calculate gradient \Delta f_i(w_t)
    Get all gradients in workers and get \sum \Delta f_i(w_t)
    Update w_{t+1} = w_t - \eta/K \sum \Delta f_i(w_t)
</code></pre>
<p>This method is suit for big computation of every mini-batch. If the batch is too small it would cost a lot of time to communicate. Also, the optimal batch size of neural network is not determined. If we want to use SSGD for optimization, we should consider the trade-off between time and accuracy.</p>
<h4 id="ma">MA</h4>
<pre><code class="lang-pseudocode">Initialize: parameter w_0, number of workers K, epochs T, gapping M, learning rates \eta
for t = 0, 1, ..., T - 1:
    Pull w_t
    for m = 0, 1, ... M:
        Random sampling mini-batch data i_m
        Calculate gradient \Delta f_i_m(w_t)
        Update w_t = w_t - \eta \Delta f_i_m(w_t)
    Get all parameters in workers and get 1/K \sum w_t
    Update w_{t+1} = 1/K \sum w_t
</code></pre>
<p>e.g. <strong>CNTK</strong></p>
<h4 id="bmuf">BMUF</h4>
<p>Adding a global momentum</p>
<pre><code>Initialize: parameter w_0, number of workers K, epochs T, gapping M, learning rates \eta, momentum \mu, \delta
for t = 0, 1, ..., T - 1:
    Pull w_t
    for m = 0, 1, ... M:
        Random sampling mini-batch data i_m
        Calculate gradient \Delta f_i_m(w_t)
        Update w_t = w_t - \eta \Delta f_i_m(w_t)
    Get all parameters in workers and get \overline{w} = 1/K \sum w_t
    Compute \Delta_t = \mu \Delta_{t-1} + \delta (\overline{w} - w_t)
    Update  w_{t+1} = w_t + \Delta_t
</code></pre><h4 id="admm">ADMM</h4>
<p>The problem of data parallelism distributed optimization can be described as:
<img src="https://latex.codecogs.com/gif.latex?%0Amin_w%20%5Csum%5EK%20l%5Ek%28w%29%0A" alt="formula">
ADMM introduces a variable z to control the difference among <img src="https://latex.codecogs.com/gif.latex?w%5Ek" alt="formula">, i.e. converting this problem as a constrained optimization problem
<img src="https://latex.codecogs.com/gif.latex?%0Amin_%7Bw%7D%20%5Csum%5EK%20l%5Ek%28w%5Ek%29%0A" alt="formula"></p>
<p><img src="https://latex.codecogs.com/gif.latex?%0As.t.w%5Ek-z%3D0%0A" alt="formula"></p>
<p>Thus, the optimization problem can be described as
<img src="https://latex.codecogs.com/gif.latex?%0Amin_%7Bw%7D%20%5Csum%5EK%20%28l%5Ek%28w%5Ek%29%20%2B%20%28%5Clambda%5Ek_t%29%5ET%20%28w%5Ek-z_t%29%20%2B%20%5Cfrac%7B%5Crho%7D%7B2%7D%20%7C%7Cw%5Ek-z_t%7C%7C_2%5E2%29%0A" alt="formula">
<strong>Worker</strong>:</p>
<pre><code>Initialize: parameter w_0^k, \lambda_t^k, number of workers K, epochs T
for t = 0, 1, ..., T - 1:
    Pull z_t
    Update \lambda_{t+1}^k = \lambda_t^k + \rho (w_t^k - z_t)
    Update w_{t+1}^k = argmin_w(\sum^K l^k(w^k) + (\lambda^k_t)^T (w^k-z_t) + \frac{\rho}{2} ||w^k-z_t||_2^2)
    Push w_{t+1}^k, \lambda_{t+1}^k
</code></pre><p><strong>Master</strong>:</p>
<pre><code>Initialize: parameter z_0, number of workers K
while True:
    while True:
        Waiting w_{t+1}^k, \lambda_{t+1}^k
        if received from all K workers: break
    Update z_{t+1} =  1/K \sum (w_t^k + \frac{1}{\rho} \lambda_t^k)
</code></pre><h4 id="easgd4">EASGD[4]</h4>
<p>The constrained optimization problem in ADMM can be simplified as 
<img src="https://latex.codecogs.com/gif.latex?%0Amin_%7Bw%7D%20%5Csum%5EK%20%28l%5Ek%28w%5Ek%29%20%2B%20%5Cfrac%7B%5Crho%7D%7B2%7D%20%7C%7Cw%5Ek-%5Coverline%7Bw%7D%7C%7C_2%5E2%29%0A" alt="formula">
we can take the first-order derivative in the update procedure.</p>
<pre><code>Initialize: global parameter w_0, local parameter w_0^k, number of workers K, epochs T, learning rates \eta, constraint ratio \alpha, \beta
for t = 0, 1, ..., T - 1:
   Random sampling mini-batch data i
   Calculate gradient \Delta f_i(w_t)
   Update w_{t+1}^k = w_t^k - \eta \Delta f_i(w_t^k) - \alpha(w_t^k - w_t)
   Get all parameters in workers and get 1/K \sum w_t
   Update global parameter w_{t+1} = (1-\beta)w_t + \beta/K \sum w_t
</code></pre><p>That is, when we update global parameters, we can take consideration of part of former parameters.</p>
<h3 id="asynchronous-algorithm">Asynchronous Algorithm</h3>
<h4 id="asgd">ASGD</h4>
<p><strong>Worker</strong>:</p>
<pre><code>Initialize: parameter w_0, number of workers K, epochs T, learning rates \eta
for t = 0, 1, ..., T - 1:
    Pull w_t^k
    Random sampling mini-batch data i
    Calculate gradient g_t^k = \Delta f_i(w_t)
    Push g_t^k
</code></pre><p><strong>Master</strong>(PS):</p>
<pre><code>while True:
    while True:
        if received g_t^k:
        break
    Update w = w - \eta g_t^k
</code></pre><p>It&#x2019;s very easy to get delay in this algorithm. The update formula can also be written as
<img src="https://latex.codecogs.com/gif.latex?%0Aw_%7Bt_1%2Bt_2%2B1%7D%3Dw_%7Bt_1%2Bt_2%7D%20-%20%5Ceta%20g%28w_%7Bt_1%7D%29%0A" alt="formula">
This would make the training unstable and may produce inaccurate result.</p>
<h4 id="aadmm">AADMM</h4>
<p><strong>Worker</strong>:</p>
<pre><code>Initialize: parameter w_0^k, \lambda_t^k, number of workers K, epochs T
for t = 0, 1, ..., T - 1:
    Pull z_t
    Update \lambda_{t+1}^k = \lambda_t^k + \rho (w_t^k - z_t)
    Update w_{t+1}^k = argmin_w(\sum^K l^k(w^k) + (\lambda^k_t)^T (w^k-z_t) + \frac{\rho}{2} ||w^k-z_t||_2^2)
    Push w_{t+1}^k, \lambda_{t+1}^k
</code></pre><p><strong>Master</strong>(PS):</p>
<pre><code>Initialize: parameter z_0, number of workers K, max delay worker K_0, max delay \tau
while True:
    while True:
        Waiting w_{t+1}^i, \lambda_{t+1}^i
        if received from worker set \Phi(which contains no less than K_0 workers) and max(\tau_1, \tau_2, ..., \tau_K_0) \le \tau: break
    for i in \Phi:
         \tau_i = 1
    for i not in \Phi:
         \tau_i = \tau_i + 1
    Update z_{t+1} =  1/K \sum_{i \in \Phi} (w_t^i + \frac{1}{\rho} \lambda_t^i)
</code></pre><h4 id="hogwild5">Hogwild![5]</h4>
<pre><code>Initialize: parameter w_0, learning rates \eta
while True:
    Random sampling mini-batch data i, using e representing the parameters related to i
    Calculate gradient g_j = \Delta_j f_i(w_t^j), j \in e
    Update w_t^j = w_t^j - \eta g_j
</code></pre><p>This is a lock-free asynchronous algorithm, which means that we don&#x2019;t need to get the write permission of w. That&#x2019;s because we can constraint the sparsity of loss function, i.e.
<img src="https://latex.codecogs.com/gif.latex?%0Al%28w%29%3D%5Csum_%7Be%5Cin%20E%7D%20f_e%28w_e%29%0A" alt="formula">
the smaller |e| and less intersection of e would make less conflicts when updating, and also guarantee the convergence of loss function.</p>
<h4 id="cyclades6">Cyclades[6]</h4>
<p>A significant benefit of the update partitioning in CYCLADES is that it induces considerable access locality compared to the more unstructured nature of the memory accesses during HOGWILD!</p>
<p><img src="img/cyclades.png" alt="cyclades"></p>
<pre><code>Initialize: Gu, T, B
Sample nb = T /B subgraphs G
Cores compute in parallel CCs for sampled subgraphs
for batch i:
    Allocation of C_i to P cores
    for each core in parallel:
        for each allocated component C:
            for each update j (in order) from C:
                x_j = u_j (x_j, f_j)
</code></pre><p>That is, predefine the subgraphs then compute.</p>
<h4 id="d-psgd7">D-PSGD[7]</h4>
<p>This is also a decentralized algorithm like Cyclades etc.</p>
<pre><code>Initialize: w_0^k, weight matrix W, learning rate \eta
for t = 0, 1, ..., T - 1:
     Random sampling mini-batch data i
     Calculate gradient g_t^k = \Delta f_i(w_t)
     Pull parameters from neighbors and get w^k_{t+1/2} = \sum_j W_k^j w_t^j
     Update w^k_{t+1} = w^k_{t+1/2} - \eta \Delta f_i(w_t)
</code></pre><h4 id="adadelay8">AdaDelay[8]</h4>
<p>This algorithm penalizes parameter updates when the nodes have delays. Assume the epoch is t and the delay is <img src="https://latex.codecogs.com/gif.latex?%5Ctau_t" alt="formula">, then the step size <img src="https://latex.codecogs.com/gif.latex?%5Calpha%28t%2C%20%5Ctau_t%29" alt="formula"> is
<img src="https://latex.codecogs.com/gif.latex?%0A%5Calpha%28t%2C%20%5Ctau_t%29%20%3D%20%28L%20%2B%20%5Ceta%28t%2C%20%5Ctau_t%29%29%5E%7B-1%7D%0A" alt="formula">
where
<img src="https://latex.codecogs.com/gif.latex?%0A%5Ceta%28t%2C%20%5Ctau_t%29%20%3D%20c%20%5Csqrt%7Bt%20%2B%20%5Ctau_t%7D%0A" alt="formula">
When <img src="https://latex.codecogs.com/gif.latex?c%20%3D%200" alt="formula">, the AdaDelay algorithm would become SGD with linear weight decay. In AdaDelay, the c is adaptive, i.e.
<img src="https://latex.codecogs.com/gif.latex?%0A%5Ceta_j%20%28t%2C%20%5Ctau_t%29%20%3D%20c_j%20%5Csqrt%7Bt%20%2B%20%5Ctau_t%7D%0A" alt="formula"></p>
<p><img src="https://latex.codecogs.com/gif.latex?%0Ac_j%20%3D%20%5Csqrt%7B%5Cfrac%7B1%7D%7Bt%7D%20%5Csum_%7Bs%3D1%7D%5Et%20%5Cfrac%7Bs%7D%7Bs%2B%5Ctau_s%7Dg_j%5E2%20%28s-%5Ctau_s%29%20%7D%0A" alt="formula"></p>
<p>the weight average of historical gradient.</p>
<p>There some similar algorithms:</p>
<p><strong>AsyncAdaGrad</strong>:
<img src="https://latex.codecogs.com/gif.latex?%0A%5Ceta_j%20%28t%2C%20%5Ctau_t%29%3D%5Csqrt%7B%5Csum_%7Bs%3D1%7D%5Et%20g_j%5E2%20%28s%2C%20%5Ctau_s%29%7D%0A" alt="formula">
<strong>AdaptiveRevision</strong>:
<img src="https://latex.codecogs.com/gif.latex?%0A%5Ceta_j%20%28t%2C%20%5Ctau_t%29%3D%5Csqrt%7B%5Csum_%7Bs%3D1%7D%5Et%20g_j%5E2%20%28s%2C%20%5Ctau_s%29%20%2B%202g_j%28t%2C%5Ctau_t%29%5Csum_%7Bs%3Dt-1%7D%5Etg_j%5E2%28s%2C%5Ctau_s%29%7D%0A" alt="formula"></p>
<h4 id="group-convolution9">Group Convolution[9]</h4>
<p><img src="img/group.png" alt="group"></p>
<h4 id="distbelief1">DistBelief[1]</h4>
<p><img src="img/distbelief.png" alt="distbelief"></p>
<h2 id="framework-based-on-mapreduce">Framework Based on MapReduce</h2>
<h3 id="spark"><a href="https://spark.apache.org/docs/latest" target="_blank">Spark</a></h3>
<p><img src="img/eco.png" alt="eco"></p>
<h4 id="rdd">RDD</h4>
<p>The main abstraction Spark provides is a <em>resilient distributed dataset</em> (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. </p>
<p>RDDs support two types of operations: <em>transformations</em>, which create a new dataset from an existing one, and <em>actions</em>, which return a value to the driver program after running a computation on the dataset. For example, <code>map</code> is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, <code>reduce</code> is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel <code>reduceByKey</code> that returns a distributed dataset).</p>
<p>All transformations in Spark are <em>lazy</em>, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through <code>map</code> will be used in a <code>reduce</code> and return only the result of the <code>reduce</code> to the driver, rather than the larger mapped dataset.</p>
<p><img src="img/rdd.png" alt="rdd"></p>
<h4 id="structured-streaming">Structured Streaming</h4>
<p>The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended. This leads to a new stream processing model that is very similar to a batch processing model. You will express your streaming computation as standard batch-like query as on a static table, and Spark runs it as an <em>incremental</em> query on the <em>unbounded</em> input table. Let&#x2019;s understand this model in more detail.</p>
<p><img src="img/stream.png" alt="stream"></p>
<p><img src="img/stream-tablel.png" alt="stream-tablel"></p>
<h4 id="distributed-components">Distributed Components</h4>
<p>The SparkContext can connect to several types of <em>cluster managers</em> (either Spark&#x2019;s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires <em>executors</em> on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends <em>tasks</em> to the executors to run.</p>
<p><img src="img/spark-architecture.png" alt="spark-architecture"></p>
<h2 id="framework-based-on-differentiable-dataflow">Framework Based on Differentiable Dataflow</h2>
<h3 id="tensorflow">TensorFlow</h3>
<p>We mainly focus on TensorFlow2. </p>
<h4 id="gradienttape">GradientTape</h4>
<p>TensorFlow provides the <code>tf.GradientTape</code> API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually <code>tf.Variable</code>s. TensorFlow &quot;records&quot; relevant operations executed inside the context of a <code>tf.GradientTape</code> onto a &quot;tape&quot;. TensorFlow then uses that tape to compute the gradients of a &quot;recorded&quot; computation using reverse mode differentiation.</p>
<h4 id="graphs">Graphs</h4>
<p>Graphs are data structures that contain a set of <code>tf.Operation</code> objects, which represent units of computation; and <code>tf.Tensor</code> objects, which represent the units of data that flow between operations. They are defined in a <code>tf.Graph</code> context.</p>
<p><img src="img/tfgraph.png" alt="tfgraph"></p>
<h4 id="architecture-general-view">Architecture General View</h4>
<p>A general view of TensorFlow architecture can be seen as a Client-Master-Worker distributed architecture.</p>
<p><img src="img/tensorflow.png" alt="tensorflow"></p>
<ul>
<li><p><strong>Client</strong></p>
<p>The client creates a session, which sends the graph definition to the distributed master as a <code>tf.GraphDef</code> protocol buffer. When the client evaluates a node or nodes in the graph, the evaluation triggers a call to the distributed master to initiate computation.</p>
</li>
<li><p><strong>Distributed Master</strong></p>
<ul>
<li>prunes the graph to obtain the subgraph required to evaluate the nodes requested by the client,</li>
<li>partitions the graph to obtain graph pieces for each participating device, and</li>
<li>caches these pieces so that they may be re-used in subsequent steps.</li>
</ul>
</li>
<li><p><strong>Worker Services</strong></p>
<ul>
<li><p>handles requests from the master,</p>
</li>
<li><p>schedules the execution of the kernels for the operations that comprise a local subgraph, and</p>
</li>
<li><p>mediates direct communication between tasks.</p>
</li>
<li><p>dispatches kernels to local devices and runs kernels in parallel when possible, for example by using multiple CPU cores or GPU streams.</p>
</li>
<li><p>Send and Recv operations for each pair of source and destination device types:</p>
<ul>
<li>Transfers between local CPU and GPU devices use the <code>cudaMemcpyAsync()</code> API to overlap computation and data transfer.</li>
<li>Transfers between two local GPUs use peer-to-peer DMA, to avoid an expensive copy via the host CPU.</li>
</ul>
<p>For transfers between tasks, mainly uses multiple protocols, including:</p>
<ul>
<li>gRPC over TCP.(CPU&amp;GPU)</li>
<li>RDMA over Converged Ethernet.(GPUs)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>One way to enable workers is to allocate one for task and the other one for parameter storing and updating. This particular division of labor between tasks is not required, but is common for distributed training.</p>
<p><img src="img/workers.png" alt="workers"></p>
<p><img src="img/clients.png" alt="clients"></p>
<p><img src="img/partition.png" alt="partition"></p>
<p><img src="img/register.png" alt="register"></p>
<p><img src="img/recv.png" alt="recv"></p>
<h4 id="between-graph-and-in-graph">Between-Graph and In-Graph</h4>
<p><img src="img/graph.png" alt="graph"></p>
<p>For <strong>in-graph replication</strong> (Figure 1(a)), a single client binary constructs one replica on each device (all contained in a massive multi-device graph), each sharing a set of parameters (resources) placed on the client&#x2019;s own device. That clients master service then coordinates synchronous execution of all replicas: feeding data and fetching outputs. (<strong>Mainly used for one machine with many GPUs</strong>)</p>
<p>For <strong>between-graph replication</strong> (Figure 1(b)), multiple machines run a client binary, each creating a graph with a single replica on a local device, alongside resources on a shared device (i.e. the parameter server model). Because this relies on TensorFlows name resolution to share state, care must be taken to ensure each client creates resources identically.</p>
<h2 id="framework-containing-actor">Framework Containing Actor</h2>
<h3 id="ray">Ray</h3>
<p><strong>Future</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> ray
ray.init()

<span class="hljs-meta">@ray.remote</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f</span><span class="hljs-params">(x)</span>:</span>
    <span class="hljs-keyword">return</span> x * x

futures = [f.remote(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>)]
print(ray.get(futures)) <span class="hljs-comment"># [0, 1, 4, 9]</span>
</code></pre>
<p><strong>Actor</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> ray
ray.init() <span class="hljs-comment"># Only call this once.</span>

<span class="hljs-meta">@ray.remote</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Counter</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        self.n = <span class="hljs-number">0</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">increment</span><span class="hljs-params">(self)</span>:</span>
        self.n += <span class="hljs-number">1</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.n

counters = [Counter.remote() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>)]
[c.increment.remote() <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> counters]
futures = [c.read.remote() <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> counters]
print(ray.get(futures)) <span class="hljs-comment"># [1, 1, 1, 1]</span>
</code></pre>
<h4 id="architecture">Architecture</h4>
<p><img src="img/ray-architecture.png" alt="ray-architecture"></p>
<p><em>Task</em> - A single function invocation that executes on a process different from the caller. A task can be stateless (a <code>@ray.remote</code> function) or stateful (a method of a <code>@ray.remote</code> class - see <em>Actor</em> below). A task is executed asynchronously with the caller: the <code>.remote()</code> call immediately returns an <code>ObjectRef</code> that can be used to retrieve the return value.</p>
<p><em>Object</em> - An application value. This may be returned by a task or created through <code>ray.put</code>. Objects are <strong>immutable</strong>: they cannot be modified once created. A worker can refer to an object using an <code>ObjectRef</code>.</p>
<p><em>Actor</em> - a stateful worker process (an instance of a <code>@ray.remote</code> class). Actor tasks must be submitted with a <em>handle</em>, or a Python reference to a specific instance of an actor.</p>
<p><em>Driver</em> - The program root. This is the code that runs <code>ray.init()</code>.</p>
<p><em>Job</em> - The collection of tasks, objects, and actors originating (recursively) from the same driver.</p>
<p>A Ray instance consists of one or more worker <strong>nodes</strong>, each of which consists of the following physical processes:</p>
<ul>
<li>One or more <strong>worker processes</strong>, responsible for task submission and execution. A worker process is either stateless (can execute any @ray.remote function) or an actor (can only execute methods according to its @ray.remote class). Each worker process is associated with a specific job. The default number of initial workers is equal to the number of CPUs on the machine. Each worker stores:<ul>
<li>An <strong>ownership table</strong>. System metadata for the objects to which the worker has a reference, e.g., to store ref counts.</li>
<li>An <strong><em>in-process store</em></strong>, used to store small objects.</li>
</ul>
</li>
<li>A <strong>raylet</strong>. The raylet is shared among all jobs on the same cluster. The raylet has two main threads:<ul>
<li>A <strong>scheduler</strong>. Responsible for resource management and fulfilling task arguments that are stored in the distributed object store. The individual schedulers in a cluster comprise the Ray <strong>distributed scheduler</strong>.</li>
<li>A <strong>shared-memory object store (also known as the</strong> <a href="https://docs.ray.io/en/stable/plasma-object-store.html" target="_blank"><strong>Plasma Object Store</strong></a><strong>)</strong>. Responsible for storing and transferring large objects. The individual object stores in a cluster comprise the Ray <strong>distributed object store</strong>.</li>
</ul>
</li>
</ul>
<p>Each worker process and raylet is assigned a unique 20-byte identifier and an IP address and port. The same address and port can be reused by subsequent components (e.g., if a previous worker process dies), but the unique IDs are never reused (i.e., they are tombstoned upon process death). Worker processes fate-share with their local raylet process.</p>
<p>One of the worker nodes is designated as the <strong>head node</strong>. In addition to the above processes, the head node also hosts:</p>
<ul>
<li>The <strong>Global Control Store (GCS)</strong>. The GCS is a key-value server that contains system-level metadata, such as the locations of objects and actors. There is an ongoing effort to support high availability for the GCS, so that it may run on any and multiple nodes, instead of a designated head node.</li>
<li>The <strong>driver process(es)</strong>. A driver is a special worker process that executes the top-level application (e.g., <code>__main__</code> in Python). It can submit tasks, but cannot execute any itself. Driver processes can run on any node, but by default are located on the head node when running with the Ray autoscaler.</li>
</ul>
<p>Protocol overview (mostly over gRPC):</p>
<ul>
<li>a: Task execution, object reference counting.</li>
<li>b: Local resource management.</li>
<li>c: Remote/distributed resource management.</li>
<li>d: Distributed object transfer.</li>
<li>e: Location lookup for objects stored in distributed object store.</li>
<li>f: Storage and retrieval of large objects. Retrieval is via <code>ray.get</code> or during task execution, when replacing a task&#x2019;s ObjectID argument with the object&#x2019;s value.</li>
<li>g: Scheduler fetches objects from remote nodes to fulfill dependencies of locally queued tasks.</li>
</ul>
<h4 id="ownership">Ownership</h4>
<p><img src="img/ray-graph.jpg" alt="img"></p>
<p>Most of the system metadata is managed according to a decentralized concept called <em>ownership</em>: Each worker process manages and <em>owns</em> the tasks that it submits and the <code>ObjectRef</code>s returned by those tasks. The owner is responsible for ensuring execution of the task and facilitating the resolution of an <code>ObjectRef</code> to its underlying value. Similarly, a worker owns any objects that it created through a <code>ray.put</code> call.</p>
<h4 id="lifetime-of-a-task">Lifetime of a Task</h4>
<p><img src="img/ray-worker.png" alt="worker"></p>
<p>When a task is submitted, the owner waits for any dependencies, i.e. <code>ObjectRef</code>s that were passed as an argument to the task (see Lifetime of an Object), to become available. Note that the dependencies need not be local; the owner considers the dependencies to be ready as soon as they are available anywhere in the cluster. When the dependencies are ready, the owner requests resources from the distributed scheduler to execute the task. Once resources are available, the scheduler grants the request and responds with the address of a worker that is now leased to the owner.</p>
<p>The owner schedules the task by sending the task specification over gRPC to the leased worker. After executing the task, the worker must store the return values. If the return values are small, the worker returns the values inline directly to the owner, which copies them to its in-process object store. If the return values are large, the worker stores the objects in its local shared memory store and replies to the owner indicating that the objects are now in distributed memory. This allows the owner to refer to the objects without having to fetch the objects to its local node.</p>
<p>When a task is submitted with an <code>ObjectRef</code> as its argument, the object value must be resolved before the worker can begin execution. If the value is small, it is copied directly from the owner&#x2019;s in-process object store into the task description, where it can be referenced by the executing worker. If the value is large, the object must be fetched from distributed memory, so that the worker has a copy in its local shared memory store. The scheduler coordinates this object transfer by looking up the object&#x2019;s locations and requesting a copy from a different node.</p>
<h4 id="lifetime-of-an-object">Lifetime of an Object</h4>
<p><img src="img/ray-object.png" alt="object"></p>
<p>The owner of an object is the worker that created the initial <code>ObjectRef</code>, by submitting the creating task or calling <code>ray.put</code>. The owner manages the lifetime of the object. Ray guarantees that if the owner is alive, the object may eventually be resolved to its value (or an error is thrown in the case of worker failure). If the owner is dead, an attempt to get the object&#x2019;s value will never hang but may throw an exception, even if there are still physical copies of the object.</p>
<p>Each worker stores a ref count for the objects that it owns. See Reference Counting for more information on how references are tracked. References are only counted during these operations:</p>
<ul>
<li>Passing an <code>ObjectRef</code> or an object that contains an <code>ObjectRef</code> as an argument to a task.</li>
<li>Returning an <code>ObjectRef</code> or an object that contains an <code>ObjectRef</code> from a task.</li>
</ul>
<p>Objects can be stored in the owner&#x2019;s in-process memory store or in the distributed object store. This decision is meant to reduce the memory footprint and resolution time for each object.</p>
<h4 id="lifetime-of-an-actor">Lifetime of an Actor</h4>
<p><img src="img/ray-actor.png" alt="actor"></p>
<p>When an actor is created in Python, the creating worker first synchronously registers the actor with the GCS. This ensures correctness in case the creating worker fails before the actor can be created. Once the GCS responds, the remainder of the actor creation process is asynchronous. The creating worker process queues locally a special task known as the actor creation task. This is similar to a normal non-actor task, except that its specified resources are acquired for the lifetime of the actor process. The creator asynchronously resolves the dependencies for the actor creation task, then sends it to the GCS service to be scheduled. Meanwhile, the Python call to create the actor immediately returns an &#x201C;actor handle&#x201D; that can be used even if the actor creation task has not yet been scheduled. See Actor Creation for more details.</p>
<p>Task execution for actors is similar to that of normal tasks: they return futures, are submitted directly to the actor process via gRPC, and will not run until all <code>ObjectRef</code> dependencies have been resolved. There are two main differences:</p>
<ul>
<li>Resources do not need to be acquired from the scheduler to execute an actor task. This is because the actor has already been granted resources for its lifetime, when its creation task was scheduled.</li>
<li>For each caller of an actor, the tasks are executed in the same order that they are submitted.</li>
</ul>
<h4 id="object-management">Object Management</h4>
<p><img src="img/ray-object-manage.png" alt="ray-object-manage"></p>
<p>In general, small objects are stored in their owner&#x2019;s <strong>in-process store</strong> while large objects are stored in the <strong>distributed object store</strong>. This decision is meant to reduce the memory footprint and resolution time for each object. Note that in the latter case, a placeholder object is stored in the in-process store to indicate the object has been <em>promoted to shared memory</em>.</p>
<p>Objects in the in-process store can be resolved quickly through a direct memory copy but may have a higher memory footprint when referenced by many processes due to the additional copies. The capacity of a single worker&#x2019;s in-process store is also limited to the memory capacity of that machine, limiting the total number of such objects that can be in reference at any given time. For objects that are referenced many times, throughput may also be limited by the processing capacity of the owner process.</p>
<p>In contrast, resolution of an object in the distributed object store requires at least one IPC from the worker to the worker&#x2019;s local shared memory store. Additional RPCs may be required if the worker&#x2019;s local shared memory store does not yet contain a copy of the object. On the other hand, because the shared memory store is implemented with shared memory, multiple workers on the same node can reference the same copy of an object. This can reduce the overall memory footprint if an object can be <a href="https://docs.ray.io/en/master/serialization.html#serialization" target="_blank">deserialized with zero copies</a>. The use of distributed memory also allows a process to reference an object without having the object local, meaning that a process can reference objects whose total size exceeds the memory capacity of a single machine. Finally, throughput can scale with the number of nodes in the distributed object store, as multiple copies of an object may be stored at different nodes.</p>
<p><img src="img/ray-resolution.png" alt="ray-resolution"></p>
<p>Resolving a large object. The object x is initially created on Node 2, e.g., because the task that returned the value ran on that node. This shows the steps when the owner (the caller of the task) calls <code>ray.get</code>: 1) Lookup object&#x2019;s locations in the GCS. 2) Select a location and send a request for a copy of the object. 3) Receive the object.</p>
<p><img src="img/ray-memory.png" alt="ray-memory"></p>
<p><em>Primary copy versus evictable copies. The primary copy (Node 2) is ineligible for eviction. However, the copies on Nodes 1 (created through <code>ray.get</code>) and 3 (created through task submission) can be evicted under memory pressure.</em></p>
<h4 id="resource-management-and-scheduling">Resource Management and Scheduling</h4>
<p>A resource in Ray is any &#x201C;key&#x201D; -&gt; float quantity. For convenience, the Ray scheduler has native support for CPU, GPU, and memory resource types, meaning that Ray automatically detects physical availability of these resources on each node. However, the user may also define custom resource requirements using any valid string, e.g., specifying a resource requirement of {&#x201C;something&#x201D;: 1}.</p>
<p><img src="img/ray-task.png" alt="img"></p>
<p>The owner waits for all task arguments to be created before requesting resources from the distributed scheduler. For example, for a program like <code>foo.remote(bar.remote())</code>, the owner will not schedule <code>foo</code> until <code>bar</code> has completed.</p>
<p>An owner schedules a task by first sending a resource request to its local raylet. The raylet queues the request and if it chooses to grant the resources, responds to the owner with the address of a local worker that is now <em>leased</em> to the owner. The lease remains active as long as the owner and leased worker are alive, and the raylet ensures that no other client may use the worker while the lease is active. To ensure fairness, an owner returns the worker if no work remains or if enough time has passed (e.g., a few hundred milliseconds).</p>
<p><img src="img/ray-schedule.png" alt="img"></p>
<h4 id="actor-management">Actor management</h4>
<p><img src="img/ray-actor-schedule.png" alt="img"></p>
<p>When an actor is created in Python, the creating worker first synchronously registers the actor with the GCS. This ensures that in the case that the creating worker dies before the actor can be created, any workers with a reference to the actor will be able to discover the failure.</p>
<p>Once all of the input dependencies for an actor creation task are resolved, the creator then sends the task specification to the GCS service. The GCS service then schedules the actor creation task through the same <a href="https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview#heading=h.jamwh5yjnwj9" target="_blank">distributed scheduling protocol</a> that is used for normal tasks, as if the GCS were the actor creation task&#x2019;s owner. Because the GCS service persists all state to the backing store, once the task specification has successfully been sent to the GCS service, the actor will eventually be created.</p>
<p>[1] <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/large_deep_networks_nips2012.pdf" target="_blank">DistBelief: Large Scale Distributed Deep Networks</a></p>
<p>[2] <a href="https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/" target="_blank">MPI Reduce and Allreduce</a></p>
<p>[3] <a href="https://github.com/microsoft/Multiverso" target="_blank">Multiverso</a></p>
<p>[4] <a href="https://arxiv.org/pdf/1412.6651.pdf" target="_blank">EASGD: Deep learning with Elastic Averaging SGD</a></p>
<p>[5] <a href="https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf" target="_blank">Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</a></p>
<p>[6] <a href="https://arxiv.org/pdf/1605.09721.pdf" target="_blank">Cyclades: Conflict-free Asynchronous Machine Learning</a></p>
<p>[7] <a href="https://arxiv.org/pdf/1705.09056.pdf" target="_blank">D-PSGD: Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent</a></p>
<p>[8] <a href="https://arxiv.org/pdf/1508.05003.pdf" target="_blank">AdaDelay: Delay Adaptive Distributed Stochastic Convex Optimization</a></p>
<p>[9] <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank">Alexnet: ImageNet Classification with Deep Convolutional Neural Networks</a></p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="./" class="navigation navigation-prev navigation-unique" aria-label="Previous page: SYSTEM">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"distributed machine learning","level":"7.1.1","depth":2,"previous":{"title":"SYSTEM","level":"7.1","depth":1,"path":"system/README.md","ref":"system/README.md","articles":[{"title":"distributed machine learning","level":"7.1.1","depth":2,"path":"system/distributed-machine-learning.md","ref":"system/distributed-machine-learning.md","articles":[]}]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"system/distributed-machine-learning.md","mtime":"2020-10-21T15:23:02.154Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-10-21T15:23:27.111Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

